# ğŸ“¦ Dispatch Monitoring System

An intelligent monitoring system for tracking and classifying items in a commercial kitchen dispatch area. Built with YOLOv8, DeepSORT, and a classification model. Feedback loop is integrated to improve model accuracy over time using user corrections and MLflow tracking.

---

## ğŸš€ Features

- âœ… **Real-time object detection** using YOLOv8
- âœ… **Object tracking** with Deep SORT
- âœ… **Item classification** (`empty`, `not_empty`, `kakigori`)
- âœ… **User feedback UI** via Streamlit
- âœ… **Retrain pipeline** based on feedback
- âœ… **MLflow integration** for experiment tracking

---

## ğŸ“ Project Structure

```

dispatch\_monitoring\_project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Original video/data
â”‚   â”œâ”€â”€ processed/         # Frame-by-frame or cropped data
â”‚   â””â”€â”€ feedback/          # User feedback JSON/CSV
â”‚
â”œâ”€â”€ models/                # Trained detection/classification models
â”œâ”€â”€ notebooks/             # Exploratory notebooks
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ detection.py       # YOLOv8 inference
â”‚   â”œâ”€â”€ tracking.py        # DeepSORT tracking
â”‚   â”œâ”€â”€ classify.py        # Classifier inference
â”‚   â”œâ”€â”€ feedback\_ui.py     # Streamlit UI
â”‚   â””â”€â”€ retrain.py         # Retraining logic
â”‚
â”œâ”€â”€ app.py                 # Run full pipeline
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ environment.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

````

---

## âš™ï¸ Installation

### ğŸ“Œ Option 1: Local (Conda)

```bash
conda env create -f environment.yaml
conda activate dispatch-monitoring-env
streamlit run app.py
````

### ğŸ“Œ Option 2: Docker

```bash
# Build & run using Docker
docker build -t dispatch-monitoring-app .
docker run -p 8501:8501 dispatch-monitoring-app
```

### ğŸ“Œ Option 3: Docker Compose (App + MLflow)

```bash
docker-compose up --build
```

---

## ğŸ§  How It Works

1. **Detection**: YOLOv8 model detects trays/plates in each frame.
2. **Tracking**: DeepSORT assigns unique IDs to follow each object.
3. **Classification**: Each object is cropped and passed to a classifier.
4. **Feedback**: User corrects predictions via UI. Feedback is stored.
5. **Retrain**: When enough feedback is collected, retrain classifier using `src/retrain.py`.
6. **Track Models**: MLflow logs each version of model and metrics.

---

## ğŸ“ Feedback Example (JSON)

```json
{
  "object_id": "12",
  "frame_id": "frame_1050",
  "predicted_label": "empty",
  "corrected_label": "not_empty"
}
```

---

## ğŸ“Š MLflow Tracking

* Access MLflow UI: [http://localhost:5000](http://localhost:5000)
* Logs:

  * model versions
  * accuracy
  * number of feedback samples used for retraining

---

## ğŸ“Œ Todo / Improvements

* [ ] Improve classifier performance with augmentations
* [ ] Replace Streamlit UI with web app (Flask or FastAPI)
* [ ] Add CI/CD pipeline for auto-retrain & redeploy
* [ ] Add GCP / AWS deployment option

---

## ğŸ‘¨â€ğŸ’» Author

**Nguyá»…n Quang Triá»u**

